import os
import uuid
import wave
import pyaudio
from gtts import gTTS
import pygame
import re
from flask import Flask, jsonify
from google.cloud import storage
import openai
from flask_cors import CORS
import librosa
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

app = Flask(__name__)
CORS(app)

# 수정 필요 OpenAI API 키
openai.api_key = ''

# GCP 인증 경로
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "turnkey-channel-454904-b0-33a5c7635740.json"

BUCKET_NAME = "a_sample"

# 녹음 설정
CHUNK = 1024
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
RECORD_SECONDS = 10

# ————— 파일명 안전 처리 함수 —————
def sanitize(text: str) -> str:
    """파일명으로 안전하게 사용할 수 있도록 변환 (<> 제외)"""
    text = text.strip()
    # 허용되지 않는 문자: \ / * ? : " < > | 
    text = re.sub(r'[\\/*?:"<>|]', "_", text)
    return text or "untitled"

# ————— GCS 업로드 함수 —————
def upload_to_gcs(local_path: str, blob_name: str):
    client = storage.Client()
    bucket = client.bucket(BUCKET_NAME)
    blob = bucket.blob(blob_name)
    blob.upload_from_filename(local_path)
    print(f"✅ GCS 업로드: gs://{BUCKET_NAME}/{blob_name}")

# ————— TTS 생성·재생·업로드 —————
def tts_and_play(text: str, safe: str):
    # TTS 파일명
    tts_filename = f"EX_{safe}.mp3"
    local_tts = os.path.join(os.path.expanduser("~"), tts_filename)
    # 1) TTS 변환
    tts = gTTS(text=text, lang='ko')
    tts.save(local_tts)
    print(f"🔊 TTS 저장: {local_tts}")

    # 2) GCS 업로드
    upload_to_gcs(local_tts, tts_filename)

    # 3) 스피커 재생
    pygame.mixer.init()
    pygame.mixer.music.load(local_tts)
    pygame.mixer.music.play()
    while pygame.mixer.music.get_busy():
        pass
    pygame.mixer.quit()
    print("🔈 재생 완료")

# ————— 녹음·저장·업로드 —————
def record_and_upload(safe: str):
    rec_filename = f"{safe}.wav"
    local_rec = os.path.join(os.path.expanduser("~"), rec_filename)
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK)
    print(f"🎙️ {RECORD_SECONDS}초간 녹음 시작...")
    frames = [stream.read(CHUNK) for _ in range(int(RATE/CHUNK*RECORD_SECONDS))]
    print("⏹ 녹음 완료")

    stream.stop_stream()
    stream.close()
    p.terminate()

    # WAV로 저장
    with wave.open(local_rec, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))
    print(f"💾 녹음 파일 저장: {local_rec}")

    # GCS 업로드
    upload_to_gcs(local_rec, rec_filename)






# 수정 필요 오디오 데이터 폴더 경로
audio_dir = '앵무새 음성파일'

# 🎵 Mel-spectrogram 추출 함수
def extract_features(audio_path, target_length=130):
    y, sr = librosa.load(audio_path, sr=None, mono=True)
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
    mel_spec_resized = librosa.util.fix_length(mel_spec, size=target_length, axis=-1)
    mel_spec_db = librosa.power_to_db(mel_spec_resized, ref=np.max)
    return mel_spec_db

# 📚 데이터 로딩
def load_data(audio_dir):
    X, y = [], []
    for label in os.listdir(audio_dir):
        label_dir = os.path.join(audio_dir, label)
        if os.path.isdir(label_dir):
            for filename in os.listdir(label_dir):
                if filename.endswith('.wav'):
                    features = extract_features(os.path.join(label_dir, filename))
                    X.append(features)
                    y.append(label)
    return np.array(X), np.array(y)

# 🧠 CNN 모델 학습
def train_and_save_model():
    X, y = load_data(audio_dir)
    print(f"✅ Loaded {len(X)} samples")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    X = X[..., np.newaxis]
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    model = Sequential([
        Conv2D(64, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(len(np.unique(y_encoded)), activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))
    model.save('parrot_speech_model.h5')
    print("✅ Model trained and saved as 'parrot_speech_model.h5'")
    return le

# 🔍 오디오 분류 및 정확도 계산
def classify_new_audio(model, audio_path, label_encoder):
    features = extract_features(audio_path)
    features = np.expand_dims(features[..., np.newaxis], axis=0)
    prediction = model.predict(features)
    predicted_idx = np.argmax(prediction, axis=1)
    predicted_label = label_encoder.inverse_transform(predicted_idx)[0]
    confidence = float(np.max(prediction))  # 가장 높은 확률 (0~1)
    return predicted_label, confidence

# 💬 GPT 피드백 생성
def get_feedback_from_gpt(word, accuracy):
    percent = int(accuracy * 100) 
    prompt = (
        f"앵무새가 '{word}' 라는 단어를 발음했습니다. "
        f"AI 모델이 판단한 발음 정확도는 {percent}%입니다. "
        f"이 정보를 바탕으로 앵무새에게 줄 수 있는 간단하고 친근한 피드백 문장을 작성해주세요. "
        f"예를 들어 '발음이 아주 좋아요!', '조금만 더 연습해볼까요?' 같은 형식으로요."
    )

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "system", "content": "You are a helpful assistant."},
                  {"role": "user", "content": prompt}],
        max_tokens=60,
        temperature=0.7,
        n=1,
    )
    
    # 피드백 처리
    try:
        # 'choices'와 'message' 존재 여부 확인
        feedback = response['choices'][0]['message']['content'].strip()
    except (KeyError, IndexError):
        feedback = "피드백을 받을 수 없습니다. 다시 시도해주세요."
    
    return feedback






if __name__ == "__main__":
    try:
        # 1. 텍스트 입력
        raw_text = input("▶ TTS로 변환할 텍스트를 입력하세요: ").strip()
        if not raw_text:
            print("❗ 텍스트가 비어 있습니다. 종료합니다.")
            exit(0)

        safe_text = sanitize(raw_text)

        # 2. TTS 생성 + 재생 + GCS 업로드
        tts_and_play(raw_text, safe_text)

        # 3. 녹음 + 저장 + GCS 업로드
        record_and_upload(safe_text)

        # 🔄 new_audio_file 경로 설정 (방금 녹음한 파일 사용)
        new_audio_file = os.path.join(os.path.expanduser("~"), f"{safe_text}.wav")

        # 4. 모델 로드
        if not os.path.exists('parrot_speech_model.h5'):
            le = train_and_save_model()
        else:
            print("✅ 모델이 이미 존재합니다. 불러오는 중...")

        import tensorflow as tf
        model = tf.keras.models.load_model('parrot_speech_model.h5')

        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        le.fit(os.listdir(audio_dir))  # 레이블 정보 불러오기

        # 5. 예측 및 피드백 실행
        predicted_label, confidence = classify_new_audio(model, new_audio_file, le)
        print(f"\n🎯 예측 결과: {predicted_label}")
        print(f"🎯 정확도: {confidence * 100:.2f}%")

        feedback = get_feedback_from_gpt(predicted_label, confidence)
        print(f"\n💬 GPT 피드백: {feedback}")

        print("✅ 모든 작업 완료.")

    except KeyboardInterrupt:
        print("\n프로그램이 중단되었습니다.")

