import os
import uuid
import wave
import pyaudio
from gtts import gTTS
import pygame
import re
from flask import Flask, jsonify
from google.cloud import storage
import openai
from flask_cors import CORS
import librosa
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

app = Flask(__name__)
CORS(app)

# ìˆ˜ì • í•„ìš” OpenAI API í‚¤
openai.api_key = ''

# GCP ì¸ì¦ ê²½ë¡œ
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "turnkey-channel-454904-b0-33a5c7635740.json"

BUCKET_NAME = "a_sample"

# ë…¹ìŒ ì„¤ì •
CHUNK = 1024
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
RECORD_SECONDS = 10

# â€”â€”â€”â€”â€” íŒŒì¼ëª… ì•ˆì „ ì²˜ë¦¬ í•¨ìˆ˜ â€”â€”â€”â€”â€”
def sanitize(text: str) -> str:
    """íŒŒì¼ëª…ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜ (<> ì œì™¸)"""
    text = text.strip()
    # í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì: \ / * ? : " < > | 
    text = re.sub(r'[\\/*?:"<>|]', "_", text)
    return text or "untitled"

# â€”â€”â€”â€”â€” GCS ì—…ë¡œë“œ í•¨ìˆ˜ â€”â€”â€”â€”â€”
def upload_to_gcs(local_path: str, blob_name: str):
    client = storage.Client()
    bucket = client.bucket(BUCKET_NAME)
    blob = bucket.blob(blob_name)
    blob.upload_from_filename(local_path)
    print(f"âœ… GCS ì—…ë¡œë“œ: gs://{BUCKET_NAME}/{blob_name}")

# â€”â€”â€”â€”â€” TTS ìƒì„±Â·ì¬ìƒÂ·ì—…ë¡œë“œ â€”â€”â€”â€”â€”
def tts_and_play(text: str, safe: str):
    # TTS íŒŒì¼ëª…
    tts_filename = f"EX_{safe}.mp3"
    local_tts = os.path.join(os.path.expanduser("~"), tts_filename)
    # 1) TTS ë³€í™˜
    tts = gTTS(text=text, lang='ko')
    tts.save(local_tts)
    print(f"ğŸ”Š TTS ì €ì¥: {local_tts}")

    # 2) GCS ì—…ë¡œë“œ
    upload_to_gcs(local_tts, tts_filename)

    # 3) ìŠ¤í”¼ì»¤ ì¬ìƒ
    pygame.mixer.init()
    pygame.mixer.music.load(local_tts)
    pygame.mixer.music.play()
    while pygame.mixer.music.get_busy():
        pass
    pygame.mixer.quit()
    print("ğŸ”ˆ ì¬ìƒ ì™„ë£Œ")

# â€”â€”â€”â€”â€” ë…¹ìŒÂ·ì €ì¥Â·ì—…ë¡œë“œ â€”â€”â€”â€”â€”
def record_and_upload(safe: str):
    rec_filename = f"{safe}.wav"
    local_rec = os.path.join(os.path.expanduser("~"), rec_filename)
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK)
    print(f"ğŸ™ï¸ {RECORD_SECONDS}ì´ˆê°„ ë…¹ìŒ ì‹œì‘...")
    frames = [stream.read(CHUNK) for _ in range(int(RATE/CHUNK*RECORD_SECONDS))]
    print("â¹ ë…¹ìŒ ì™„ë£Œ")

    stream.stop_stream()
    stream.close()
    p.terminate()

    # WAVë¡œ ì €ì¥
    with wave.open(local_rec, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))
    print(f"ğŸ’¾ ë…¹ìŒ íŒŒì¼ ì €ì¥: {local_rec}")

    # GCS ì—…ë¡œë“œ
    upload_to_gcs(local_rec, rec_filename)






# ìˆ˜ì • í•„ìš” ì˜¤ë””ì˜¤ ë°ì´í„° í´ë” ê²½ë¡œ
audio_dir = 'ì•µë¬´ìƒˆ ìŒì„±íŒŒì¼'

# ğŸµ Mel-spectrogram ì¶”ì¶œ í•¨ìˆ˜
def extract_features(audio_path, target_length=130):
    y, sr = librosa.load(audio_path, sr=None, mono=True)
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
    mel_spec_resized = librosa.util.fix_length(mel_spec, size=target_length, axis=-1)
    mel_spec_db = librosa.power_to_db(mel_spec_resized, ref=np.max)
    return mel_spec_db

# ğŸ“š ë°ì´í„° ë¡œë”©
def load_data(audio_dir):
    X, y = [], []
    for label in os.listdir(audio_dir):
        label_dir = os.path.join(audio_dir, label)
        if os.path.isdir(label_dir):
            for filename in os.listdir(label_dir):
                if filename.endswith('.wav'):
                    features = extract_features(os.path.join(label_dir, filename))
                    X.append(features)
                    y.append(label)
    return np.array(X), np.array(y)

# ğŸ§  CNN ëª¨ë¸ í•™ìŠµ
def train_and_save_model():
    X, y = load_data(audio_dir)
    print(f"âœ… Loaded {len(X)} samples")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    X = X[..., np.newaxis]
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    model = Sequential([
        Conv2D(64, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(len(np.unique(y_encoded)), activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))
    model.save('parrot_speech_model.h5')
    print("âœ… Model trained and saved as 'parrot_speech_model.h5'")
    return le

# ğŸ” ì˜¤ë””ì˜¤ ë¶„ë¥˜ ë° ì •í™•ë„ ê³„ì‚°
def classify_new_audio(model, audio_path, label_encoder):
    features = extract_features(audio_path)
    features = np.expand_dims(features[..., np.newaxis], axis=0)
    prediction = model.predict(features)
    predicted_idx = np.argmax(prediction, axis=1)
    predicted_label = label_encoder.inverse_transform(predicted_idx)[0]
    confidence = float(np.max(prediction))  # ê°€ì¥ ë†’ì€ í™•ë¥  (0~1)
    return predicted_label, confidence

# ğŸ’¬ GPT í”¼ë“œë°± ìƒì„±
def get_feedback_from_gpt(word, accuracy):
    percent = int(accuracy * 100) 
    prompt = (
        f"ì•µë¬´ìƒˆê°€ '{word}' ë¼ëŠ” ë‹¨ì–´ë¥¼ ë°œìŒí–ˆìŠµë‹ˆë‹¤. "
        f"AI ëª¨ë¸ì´ íŒë‹¨í•œ ë°œìŒ ì •í™•ë„ëŠ” {percent}%ì…ë‹ˆë‹¤. "
        f"ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•µë¬´ìƒˆì—ê²Œ ì¤„ ìˆ˜ ìˆëŠ” ê°„ë‹¨í•˜ê³  ì¹œê·¼í•œ í”¼ë“œë°± ë¬¸ì¥ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. "
        f"ì˜ˆë¥¼ ë“¤ì–´ 'ë°œìŒì´ ì•„ì£¼ ì¢‹ì•„ìš”!', 'ì¡°ê¸ˆë§Œ ë” ì—°ìŠµí•´ë³¼ê¹Œìš”?' ê°™ì€ í˜•ì‹ìœ¼ë¡œìš”."
    )

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "system", "content": "You are a helpful assistant."},
                  {"role": "user", "content": prompt}],
        max_tokens=60,
        temperature=0.7,
        n=1,
    )
    
    # í”¼ë“œë°± ì²˜ë¦¬
    try:
        # 'choices'ì™€ 'message' ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        feedback = response['choices'][0]['message']['content'].strip()
    except (KeyError, IndexError):
        feedback = "í”¼ë“œë°±ì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    return feedback






if __name__ == "__main__":
    try:
        # 1. í…ìŠ¤íŠ¸ ì…ë ¥
        raw_text = input("â–¶ TTSë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not raw_text:
            print("â— í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit(0)

        safe_text = sanitize(raw_text)

        # 2. TTS ìƒì„± + ì¬ìƒ + GCS ì—…ë¡œë“œ
        tts_and_play(raw_text, safe_text)

        # 3. ë…¹ìŒ + ì €ì¥ + GCS ì—…ë¡œë“œ
        record_and_upload(safe_text)

        # ğŸ”„ new_audio_file ê²½ë¡œ ì„¤ì • (ë°©ê¸ˆ ë…¹ìŒí•œ íŒŒì¼ ì‚¬ìš©)
        new_audio_file = os.path.join(os.path.expanduser("~"), f"{safe_text}.wav")

        # 4. ëª¨ë¸ ë¡œë“œ
        if not os.path.exists('parrot_speech_model.h5'):
            le = train_and_save_model()
        else:
            print("âœ… ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")

        import tensorflow as tf
        model = tf.keras.models.load_model('parrot_speech_model.h5')

        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        le.fit(os.listdir(audio_dir))  # ë ˆì´ë¸” ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°

        # 5. ì˜ˆì¸¡ ë° í”¼ë“œë°± ì‹¤í–‰
        predicted_label, confidence = classify_new_audio(model, new_audio_file, le)
        print(f"\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼: {predicted_label}")
        print(f"ğŸ¯ ì •í™•ë„: {confidence * 100:.2f}%")

        feedback = get_feedback_from_gpt(predicted_label, confidence)
        print(f"\nğŸ’¬ GPT í”¼ë“œë°±: {feedback}")

        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ.")

    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

