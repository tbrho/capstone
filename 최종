#스피커+마이크+음성인식+발음정확도+피드백

import os
import time
import librosa
import numpy as np
import tensorflow as tf
import openai
import pyaudio
import wave
from google.cloud import speech
from google.cloud import storage
from io import BytesIO
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# 🔑 GPT API 키 (보안상 실제 사용 시 환경변수로 관리 권장)
openai.api_key = 'sk-...'

# 📁 오디오 데이터 폴더 경로
audio_dir = '/home/pi/parrot_audio'  # 라즈베리파이 디렉토리 설정

# 🎵 Mel-spectrogram 추출 함수
def extract_features(audio_path, target_length=130):
    y, sr = librosa.load(audio_path, sr=None, mono=True)
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
    mel_spec_resized = librosa.util.fix_length(mel_spec, size=target_length, axis=-1)
    mel_spec_db = librosa.power_to_db(mel_spec_resized, ref=np.max)
    return mel_spec_db

# 📚 데이터 로딩
def load_data(audio_dir):
    X, y = [], []
    for label in os.listdir(audio_dir):
        label_dir = os.path.join(audio_dir, label)
        if os.path.isdir(label_dir):
            for filename in os.listdir(label_dir):
                if filename.endswith('.wav'):
                    features = extract_features(os.path.join(label_dir, filename))
                    X.append(features)
                    y.append(label)
    return np.array(X), np.array(y)

# 🧠 CNN 모델 학습
def train_and_save_model():
    X, y = load_data(audio_dir)
    print(f"✅ Loaded {len(X)} samples")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    X = X[..., np.newaxis]
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    model = Sequential([
        Conv2D(64, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(len(np.unique(y_encoded)), activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))
    model.save('parrot_speech_model.h5')
    print("✅ Model trained and saved as 'parrot_speech_model.h5'")
    return le

# 🔍 오디오 분류 및 정확도 계산
def classify_new_audio(model, audio_path, label_encoder):
    features = extract_features(audio_path)
    features = np.expand_dims(features[..., np.newaxis], axis=0)
    prediction = model.predict(features)
    predicted_idx = np.argmax(prediction, axis=1)
    predicted_label = label_encoder.inverse_transform(predicted_idx)[0]
    confidence = float(np.max(prediction))  # 가장 높은 확률 (0~1)
    return predicted_label, confidence

# 💬 GPT 피드백 생성
def get_feedback_from_gpt(word, accuracy):
    percent = int(accuracy * 100)
    prompt = (
        f"앵무새가 '{word}' 라는 단어를 발음했습니다. "
        f"AI 모델이 판단한 발음 정확도는 {percent}%입니다. "
        f"이 정보를 바탕으로 앵무새에게 줄 수 있는 간단하고 친근한 피드백 문장을 작성해주세요. "
        f"예를 들어 '발음이 아주 좋아요!', '조금만 더 연습해볼까요?' 같은 형식으로요."
    )

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "system", "content": "You are a helpful assistant."},
                  {"role": "user", "content": prompt}],
        max_tokens=60,
        temperature=0.7,
        n=1,
    )
    
    # 피드백 처리
    try:
        # 'choices'와 'message' 존재 여부 확인
        feedback = response['choices'][0]['message']['content'].strip()
    except (KeyError, IndexError):
        feedback = "피드백을 받을 수 없습니다. 다시 시도해주세요."
    
    return feedback


# 라즈베리파이 마이크와 스피커 설정
p = pyaudio.PyAudio()
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK_SIZE = 1024
RECORD_SECONDS = 3  # 앵무새가 발음하는 시간 (몇 초 동안 따라 말할지 설정)

# 1. 스피커 출력: TTS로 텍스트를 말하게 하기
def play_text_with_tts(text):
    import pyttsx3  # TTS 라이브러리 (사전 설치 필요)
    engine = pyttsx3.init()
    engine.setProperty('rate', 150)  # 속도
    engine.setProperty('volume', 1)  # 볼륨 (0.0 ~ 1.0)
    engine.say(text)
    engine.runAndWait()

# 2. 음성 녹음: 앵무새가 발음한 음성을 녹음
def record_audio(output_path, record_seconds=RECORD_SECONDS):
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK_SIZE)

    print("앵무새가 발음 중...")
    frames = []

    # 앵무새가 발음할 시간을 두고 녹음 시작
    time.sleep(RECORD_SECONDS)  # 발음 시간이 끝날 때까지 기다리기

    for _ in range(0, int(RATE / CHUNK_SIZE * RECORD_SECONDS)):
        data = stream.read(CHUNK_SIZE)
        frames.append(data)

    print("녹음 완료.")

    # 녹음한 데이터를 파일로 저장
    with wave.open(output_path, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))

    stream.stop_stream()
    stream.close()

# 3. 음성 파일을 GCS에 업로드
def upload_to_gcs(audio_file_path, transcript_text):
    storage_client = storage.Client()
    bucket_name = "a_sample"
    bucket = storage_client.bucket(bucket_name)

    # 텍스트 파일 업로드
    text_blob = bucket.blob(f"{transcript_text}.txt")
    text_stream = BytesIO(transcript_text.encode("utf-8"))
    text_blob.upload_from_file(text_stream, content_type="text/plain")

    # 음성 파일 업로드
    audio_blob = bucket.blob(f"{transcript_text}.wav")
    audio_stream = BytesIO(open(audio_file_path, 'rb').read())
    audio_blob.upload_from_file(audio_stream, content_type="audio/wav")

    print(f"✅ GCS에 파일 업로드 완료: {audio_file_path}, {transcript_text}.txt")

# 4. 마이크로부터 음성 인식 후 피드백 출력
def recognize_speech_from_mic(audio_path):
    client = speech.SpeechClient()

    with open(audio_path, 'rb') as audio_file:
        audio_content = audio_file.read()

    audio = speech.RecognitionAudio(content=audio_content)

    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=RATE,
        language_code="ko-KR"
    )

    response = client.recognize(config=config, audio=audio)

    # 텍스트 변환 결과
    transcript_text = ""
    for result in response.results:
        transcript_text += result.alternatives[0].transcript + "\n"
    
    print("음성 인식 결과:", transcript_text)
    return transcript_text


# ▶️ 실행 파트
if __name__ == "__main__":
    # 학습 후 모델 저장 (한 번만 실행하면 이후 생략 가능)
    if not os.path.exists('parrot_speech_model.h5'):
        le = train_and_save_model()
    else:
        print("✅ 모델이 이미 존재합니다. 불러오는 중...")

    # 모델과 레이블 로드
    model = tf.keras.models.load_model('parrot_speech_model.h5')
    le = LabelEncoder()
    le.fit(os.listdir(audio_dir))  # 레이블 정보 로드

    # 텍스트를 스피커로 출력
    text_to_speak = "안녕하세요"
    play_text_with_tts(text_to_speak)

    # 앵무새 발음 녹음
    recorded_audio_path = "/home/pi/recorded_audio.wav"
    record_audio(recorded_audio_path)

    # 녹음된 음성 인식 후 피드백 제공
    transcript_text = recognize_speech_from_mic(recorded_audio_path)

    # GPT 피드백 생성
    predicted_label, confidence = classify_new_audio(model, recorded_audio_path, le)
    print(f"\n🎯 예측 결과: {predicted_label}")
    print(f"🎯 정확도: {confidence * 100:.2f}%")
    feedback = get_feedback_from_gpt(predicted_label, confidence)
    print(f"\n💬 GPT 피드백: {feedback}")

    # 결과를 GCS에 업로드
    upload_to_gcs(recorded_audio_path, transcript_text)
