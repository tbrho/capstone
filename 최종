#ìŠ¤í”¼ì»¤+ë§ˆì´í¬+ìŒì„±ì¸ì‹+ë°œìŒì •í™•ë„+í”¼ë“œë°±

import os
import time
import librosa
import numpy as np
import tensorflow as tf
import openai
import pyaudio
import wave
from google.cloud import speech
from google.cloud import storage
from io import BytesIO
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# ğŸ”‘ GPT API í‚¤ (ë³´ì•ˆìƒ ì‹¤ì œ ì‚¬ìš© ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬ ê¶Œì¥)
openai.api_key = 'sk-...'

# ğŸ“ ì˜¤ë””ì˜¤ ë°ì´í„° í´ë” ê²½ë¡œ
audio_dir = '/home/pi/parrot_audio'  # ë¼ì¦ˆë² ë¦¬íŒŒì´ ë””ë ‰í† ë¦¬ ì„¤ì •

# ğŸµ Mel-spectrogram ì¶”ì¶œ í•¨ìˆ˜
def extract_features(audio_path, target_length=130):
    y, sr = librosa.load(audio_path, sr=None, mono=True)
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
    mel_spec_resized = librosa.util.fix_length(mel_spec, size=target_length, axis=-1)
    mel_spec_db = librosa.power_to_db(mel_spec_resized, ref=np.max)
    return mel_spec_db

# ğŸ“š ë°ì´í„° ë¡œë”©
def load_data(audio_dir):
    X, y = [], []
    for label in os.listdir(audio_dir):
        label_dir = os.path.join(audio_dir, label)
        if os.path.isdir(label_dir):
            for filename in os.listdir(label_dir):
                if filename.endswith('.wav'):
                    features = extract_features(os.path.join(label_dir, filename))
                    X.append(features)
                    y.append(label)
    return np.array(X), np.array(y)

# ğŸ§  CNN ëª¨ë¸ í•™ìŠµ
def train_and_save_model():
    X, y = load_data(audio_dir)
    print(f"âœ… Loaded {len(X)} samples")
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    X = X[..., np.newaxis]
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    model = Sequential([
        Conv2D(64, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(len(np.unique(y_encoded)), activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))
    model.save('parrot_speech_model.h5')
    print("âœ… Model trained and saved as 'parrot_speech_model.h5'")
    return le

# ğŸ” ì˜¤ë””ì˜¤ ë¶„ë¥˜ ë° ì •í™•ë„ ê³„ì‚°
def classify_new_audio(model, audio_path, label_encoder):
    features = extract_features(audio_path)
    features = np.expand_dims(features[..., np.newaxis], axis=0)
    prediction = model.predict(features)
    predicted_idx = np.argmax(prediction, axis=1)
    predicted_label = label_encoder.inverse_transform(predicted_idx)[0]
    confidence = float(np.max(prediction))  # ê°€ì¥ ë†’ì€ í™•ë¥  (0~1)
    return predicted_label, confidence

# ğŸ’¬ GPT í”¼ë“œë°± ìƒì„±
def get_feedback_from_gpt(word, accuracy):
    percent = int(accuracy * 100)
    prompt = (
        f"ì•µë¬´ìƒˆê°€ '{word}' ë¼ëŠ” ë‹¨ì–´ë¥¼ ë°œìŒí–ˆìŠµë‹ˆë‹¤. "
        f"AI ëª¨ë¸ì´ íŒë‹¨í•œ ë°œìŒ ì •í™•ë„ëŠ” {percent}%ì…ë‹ˆë‹¤. "
        f"ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•µë¬´ìƒˆì—ê²Œ ì¤„ ìˆ˜ ìˆëŠ” ê°„ë‹¨í•˜ê³  ì¹œê·¼í•œ í”¼ë“œë°± ë¬¸ì¥ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. "
        f"ì˜ˆë¥¼ ë“¤ì–´ 'ë°œìŒì´ ì•„ì£¼ ì¢‹ì•„ìš”!', 'ì¡°ê¸ˆë§Œ ë” ì—°ìŠµí•´ë³¼ê¹Œìš”?' ê°™ì€ í˜•ì‹ìœ¼ë¡œìš”."
    )

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "system", "content": "You are a helpful assistant."},
                  {"role": "user", "content": prompt}],
        max_tokens=60,
        temperature=0.7,
        n=1,
    )
    
    # í”¼ë“œë°± ì²˜ë¦¬
    try:
        # 'choices'ì™€ 'message' ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        feedback = response['choices'][0]['message']['content'].strip()
    except (KeyError, IndexError):
        feedback = "í”¼ë“œë°±ì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    return feedback


# ë¼ì¦ˆë² ë¦¬íŒŒì´ ë§ˆì´í¬ì™€ ìŠ¤í”¼ì»¤ ì„¤ì •
p = pyaudio.PyAudio()
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK_SIZE = 1024
RECORD_SECONDS = 3  # ì•µë¬´ìƒˆê°€ ë°œìŒí•˜ëŠ” ì‹œê°„ (ëª‡ ì´ˆ ë™ì•ˆ ë”°ë¼ ë§í• ì§€ ì„¤ì •)

# 1. ìŠ¤í”¼ì»¤ ì¶œë ¥: TTSë¡œ í…ìŠ¤íŠ¸ë¥¼ ë§í•˜ê²Œ í•˜ê¸°
def play_text_with_tts(text):
    import pyttsx3  # TTS ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì‚¬ì „ ì„¤ì¹˜ í•„ìš”)
    engine = pyttsx3.init()
    engine.setProperty('rate', 150)  # ì†ë„
    engine.setProperty('volume', 1)  # ë³¼ë¥¨ (0.0 ~ 1.0)
    engine.say(text)
    engine.runAndWait()

# 2. ìŒì„± ë…¹ìŒ: ì•µë¬´ìƒˆê°€ ë°œìŒí•œ ìŒì„±ì„ ë…¹ìŒ
def record_audio(output_path, record_seconds=RECORD_SECONDS):
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK_SIZE)

    print("ì•µë¬´ìƒˆê°€ ë°œìŒ ì¤‘...")
    frames = []

    # ì•µë¬´ìƒˆê°€ ë°œìŒí•  ì‹œê°„ì„ ë‘ê³  ë…¹ìŒ ì‹œì‘
    time.sleep(RECORD_SECONDS)  # ë°œìŒ ì‹œê°„ì´ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸°

    for _ in range(0, int(RATE / CHUNK_SIZE * RECORD_SECONDS)):
        data = stream.read(CHUNK_SIZE)
        frames.append(data)

    print("ë…¹ìŒ ì™„ë£Œ.")

    # ë…¹ìŒí•œ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    with wave.open(output_path, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(p.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))

    stream.stop_stream()
    stream.close()

# 3. ìŒì„± íŒŒì¼ì„ GCSì— ì—…ë¡œë“œ
def upload_to_gcs(audio_file_path, transcript_text):
    storage_client = storage.Client()
    bucket_name = "a_sample"
    bucket = storage_client.bucket(bucket_name)

    # í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ
    text_blob = bucket.blob(f"{transcript_text}.txt")
    text_stream = BytesIO(transcript_text.encode("utf-8"))
    text_blob.upload_from_file(text_stream, content_type="text/plain")

    # ìŒì„± íŒŒì¼ ì—…ë¡œë“œ
    audio_blob = bucket.blob(f"{transcript_text}.wav")
    audio_stream = BytesIO(open(audio_file_path, 'rb').read())
    audio_blob.upload_from_file(audio_stream, content_type="audio/wav")

    print(f"âœ… GCSì— íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {audio_file_path}, {transcript_text}.txt")

# 4. ë§ˆì´í¬ë¡œë¶€í„° ìŒì„± ì¸ì‹ í›„ í”¼ë“œë°± ì¶œë ¥
def recognize_speech_from_mic(audio_path):
    client = speech.SpeechClient()

    with open(audio_path, 'rb') as audio_file:
        audio_content = audio_file.read()

    audio = speech.RecognitionAudio(content=audio_content)

    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=RATE,
        language_code="ko-KR"
    )

    response = client.recognize(config=config, audio=audio)

    # í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼
    transcript_text = ""
    for result in response.results:
        transcript_text += result.alternatives[0].transcript + "\n"
    
    print("ìŒì„± ì¸ì‹ ê²°ê³¼:", transcript_text)
    return transcript_text


# â–¶ï¸ ì‹¤í–‰ íŒŒíŠ¸
if __name__ == "__main__":
    # í•™ìŠµ í›„ ëª¨ë¸ ì €ì¥ (í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë©´ ì´í›„ ìƒëµ ê°€ëŠ¥)
    if not os.path.exists('parrot_speech_model.h5'):
        le = train_and_save_model()
    else:
        print("âœ… ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")

    # ëª¨ë¸ê³¼ ë ˆì´ë¸” ë¡œë“œ
    model = tf.keras.models.load_model('parrot_speech_model.h5')
    le = LabelEncoder()
    le.fit(os.listdir(audio_dir))  # ë ˆì´ë¸” ì •ë³´ ë¡œë“œ

    # í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í”¼ì»¤ë¡œ ì¶œë ¥
    text_to_speak = "ì•ˆë…•í•˜ì„¸ìš”"
    play_text_with_tts(text_to_speak)

    # ì•µë¬´ìƒˆ ë°œìŒ ë…¹ìŒ
    recorded_audio_path = "/home/pi/recorded_audio.wav"
    record_audio(recorded_audio_path)

    # ë…¹ìŒëœ ìŒì„± ì¸ì‹ í›„ í”¼ë“œë°± ì œê³µ
    transcript_text = recognize_speech_from_mic(recorded_audio_path)

    # GPT í”¼ë“œë°± ìƒì„±
    predicted_label, confidence = classify_new_audio(model, recorded_audio_path, le)
    print(f"\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼: {predicted_label}")
    print(f"ğŸ¯ ì •í™•ë„: {confidence * 100:.2f}%")
    feedback = get_feedback_from_gpt(predicted_label, confidence)
    print(f"\nğŸ’¬ GPT í”¼ë“œë°±: {feedback}")

    # ê²°ê³¼ë¥¼ GCSì— ì—…ë¡œë“œ
    upload_to_gcs(recorded_audio_path, transcript_text)
