import os
import time
import re
import wave
import librosa
import numpy as np
import tensorflow as tf
import openai
import pyaudio
from io import BytesIO
from gtts import gTTS
from playsound import playsound
from google.cloud import storage
from sklearn.preprocessing import LabelEncoder

# í™˜ê²½ì„¤ì •
openai.api_key = 'YOUR_GPT_API_KEY'
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/home/pi/your-google-credentials.json"
bucket_name = "a_sample"

# ì´ˆê¸°í™”
RATE = 16000
CHANNELS = 1
CHUNK = 1024
FORMAT = pyaudio.paInt16
RECORD_SECONDS = 5
WAIT_SECONDS = 4  # ì•µë¬´ìƒˆê°€ ë”°ë¼ ë§í•  ì‹œê°„
audio_path = "recorded_parrot.wav"
tts_path = "tts_output.mp3"

# GCS í´ë¼ì´ì–¸íŠ¸
storage_client = storage.Client()
bucket = storage_client.bucket(bucket_name)

# CNN ëª¨ë¸ & ë¼ë²¨ ë¡œë”©
model = tf.keras.models.load_model("parrot_speech_model.h5")
label_encoder = LabelEncoder()
label_encoder.classes_ = np.load("label_classes.npy")  # ë¯¸ë¦¬ ì €ì¥ëœ í´ë˜ìŠ¤

# (1) ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
input_text = "ì•ˆë…•"

# (2) TTS ë³€í™˜ ë° ì¬ìƒ
tts = gTTS(text=input_text, lang='ko')
tts.save(tts_path)
playsound(tts_path)

# (3) ì•µë¬´ìƒˆ ëŒ€ê¸° ì‹œê°„
print(f"â³ ì•µë¬´ìƒˆê°€ ë”°ë¼ ë§í•  ì‹œê°„ ëŒ€ê¸° ì¤‘... ({WAIT_SECONDS}ì´ˆ)")
time.sleep(WAIT_SECONDS)

# (4) ë§ˆì´í¬ ë…¹ìŒ ì‹œì‘
p = pyaudio.PyAudio()
stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)
print("ğŸ™ï¸ ì•µë¬´ìƒˆ ë°œì„± ë…¹ìŒ ì¤‘...")
frames = [stream.read(CHUNK) for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS))]
print("âœ… ë…¹ìŒ ì™„ë£Œ")
stream.stop_stream()
stream.close()
p.terminate()

# (5) WAV ì €ì¥
with wave.open(audio_path, 'wb') as wf:
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))

# (6) GCS ì—…ë¡œë“œ
def sanitize_filename(text):
    return re.sub(r'[\\/*?:"<>|\n\r ]', "_", text.strip())[:50] or "untitled"

safe_name = sanitize_filename(input_text)
blob_audio = bucket.blob(f"{safe_name}.wav")
blob_audio.upload_from_filename(audio_path)
print(f"âœ… ë…¹ìŒëœ ìŒì„± ì €ì¥: gs://{bucket_name}/{safe_name}.wav")

# (7) ìŒì„± íŠ¹ì§• ì¶”ì¶œ
def extract_features(audio_file, target_length=130):
    y, sr = librosa.load(audio_file, sr=None)
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
    mel_spec = librosa.util.fix_length(mel_spec, size=target_length, axis=1)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    return mel_spec_db[..., np.newaxis][np.newaxis, ...]

features = extract_features(audio_path)

# (8) ì˜ˆì¸¡ ë° ì •í™•ë„
prediction = model.predict(features)
predicted_index = np.argmax(prediction)
predicted_word = label_encoder.inverse_transform([predicted_index])[0]
confidence = float(np.max(prediction))

print(f"\nğŸ” ì˜ˆì¸¡ ë‹¨ì–´: {predicted_word}")
print(f"ğŸ“Š ì •í™•ë„: {confidence:.2%}")

# (9) GPT í”¼ë“œë°±
def get_feedback(word, accuracy):
    percent = int(accuracy * 100)
    prompt = (
        f"ì•µë¬´ìƒˆê°€ '{word}'ë¼ëŠ” ë‹¨ì–´ë¥¼ ë°œìŒí–ˆìŠµë‹ˆë‹¤. "
        f"AI ëª¨ë¸ì´ íŒë‹¨í•œ ì •í™•ë„ëŠ” {percent}%ì…ë‹ˆë‹¤. "
        "ì´ ë°œìŒì— ëŒ€í•œ ì§§ê³  ì¹œê·¼í•œ í”¼ë“œë°± ë¬¸ì¥ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ì˜ˆ: 'ì•„ì£¼ ì˜í–ˆì–´ìš”!', 'ì¡°ê¸ˆë§Œ ë” í˜ë‚´ìš”!'"
    )

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ì¹œì ˆí•œ í”¼ë“œë°±ì„ ì£¼ëŠ” ì¡°êµ"},
            {"role": "user", "content": prompt}
        ],
        max_tokens=60,
        temperature=0.7,
    )
    return response['choices'][0]['message']['content'].strip()

feedback = get_feedback(predicted_word, confidence)
print(f"\nğŸ’¬ GPT í”¼ë“œë°±: {feedback}")
