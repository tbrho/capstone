import os
import time
import re
import wave
import librosa
import numpy as np
import tensorflow as tf
import openai
import pyaudio
from io import BytesIO
from gtts import gTTS
from playsound import playsound
from google.cloud import storage
from sklearn.preprocessing import LabelEncoder

# 환경설정
openai.api_key = 'YOUR_GPT_API_KEY'
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/home/pi/your-google-credentials.json"
bucket_name = "a_sample"

# 초기화
RATE = 16000
CHANNELS = 1
CHUNK = 1024
FORMAT = pyaudio.paInt16
RECORD_SECONDS = 5
WAIT_SECONDS = 4  # 앵무새가 따라 말할 시간
audio_path = "recorded_parrot.wav"
tts_path = "tts_output.mp3"

# GCS 클라이언트
storage_client = storage.Client()
bucket = storage_client.bucket(bucket_name)

# CNN 모델 & 라벨 로딩
model = tf.keras.models.load_model("parrot_speech_model.h5")
label_encoder = LabelEncoder()
label_encoder.classes_ = np.load("label_classes.npy")  # 미리 저장된 클래스

# (1) 사용자 입력 텍스트
input_text = "안녕"

# (2) TTS 변환 및 재생
tts = gTTS(text=input_text, lang='ko')
tts.save(tts_path)
playsound(tts_path)

# (3) 앵무새 대기 시간
print(f"⏳ 앵무새가 따라 말할 시간 대기 중... ({WAIT_SECONDS}초)")
time.sleep(WAIT_SECONDS)

# (4) 마이크 녹음 시작
p = pyaudio.PyAudio()
stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)
print("🎙️ 앵무새 발성 녹음 중...")
frames = [stream.read(CHUNK) for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS))]
print("✅ 녹음 완료")
stream.stop_stream()
stream.close()
p.terminate()

# (5) WAV 저장
with wave.open(audio_path, 'wb') as wf:
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(p.get_sample_size(FORMAT))
    wf.setframerate(RATE)
    wf.writeframes(b''.join(frames))

# (6) GCS 업로드
def sanitize_filename(text):
    return re.sub(r'[\\/*?:"<>|\n\r ]', "_", text.strip())[:50] or "untitled"

safe_name = sanitize_filename(input_text)
blob_audio = bucket.blob(f"{safe_name}.wav")
blob_audio.upload_from_filename(audio_path)
print(f"✅ 녹음된 음성 저장: gs://{bucket_name}/{safe_name}.wav")

# (7) 음성 특징 추출
def extract_features(audio_file, target_length=130):
    y, sr = librosa.load(audio_file, sr=None)
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
    mel_spec = librosa.util.fix_length(mel_spec, size=target_length, axis=1)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    return mel_spec_db[..., np.newaxis][np.newaxis, ...]

features = extract_features(audio_path)

# (8) 예측 및 정확도
prediction = model.predict(features)
predicted_index = np.argmax(prediction)
predicted_word = label_encoder.inverse_transform([predicted_index])[0]
confidence = float(np.max(prediction))

print(f"\n🔍 예측 단어: {predicted_word}")
print(f"📊 정확도: {confidence:.2%}")

# (9) GPT 피드백
def get_feedback(word, accuracy):
    percent = int(accuracy * 100)
    prompt = (
        f"앵무새가 '{word}'라는 단어를 발음했습니다. "
        f"AI 모델이 판단한 정확도는 {percent}%입니다. "
        "이 발음에 대한 짧고 친근한 피드백 문장을 생성해주세요. 예: '아주 잘했어요!', '조금만 더 힘내요!'"
    )

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "친절한 피드백을 주는 조교"},
            {"role": "user", "content": prompt}
        ],
        max_tokens=60,
        temperature=0.7,
    )
    return response['choices'][0]['message']['content'].strip()

feedback = get_feedback(predicted_word, confidence)
print(f"\n💬 GPT 피드백: {feedback}")
